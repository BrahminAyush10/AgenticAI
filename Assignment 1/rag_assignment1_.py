# -*- coding: utf-8 -*-
"""RAG_Assignment1_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Qk5VDocWhrxLZsoLFw21D6ASIe_rjEc

# Retrieval-Augmented Generation (RAG) on Research Paper

## Objective
To implement a Retrieval-Augmented Generation (RAG) pipeline on the research paper:

**"AI Driven Crop Disease Detection and Management System"**

The system will:
- Parse the PDF
- Split text into semantic chunks
- Generate embeddings
- Store vectors using FAISS
- Retrieve relevant context
- Generate grounded answers using FLAN-T5
"""

!pip install -q sentence-transformers faiss-cpu pypdf transformers accelerate

"""## Step 1: Import Required Libraries

"""

import faiss
import numpy as np
from pypdf import PdfReader
from sentence_transformers import SentenceTransformer
from transformers import pipeline

"""## Step 2: Load the Research Paper PDF

"""

reader = PdfReader("IJISRT25NOV542.pdf")

text = ""
for page in reader.pages:
    text += page.extract_text()

print("Total Characters:", len(text))

"""## Step 3: Split Text into Overlapping Chunks

Chunking improves retrieval quality by breaking long documents into smaller semantic units.

"""

def chunk_text(text, chunk_size=800, overlap=150):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks

chunks = chunk_text(text)

print("Total Chunks:", len(chunks))

"""## Step 4: Generate Embeddings

We use SentenceTransformer (all-MiniLM-L6-v2) to convert text chunks into vector representations.

"""

embedder = SentenceTransformer("all-MiniLM-L6-v2")

embeddings = embedder.encode(chunks, show_progress_bar=True)

"""## Step 5: Store Embeddings in FAISS Vector Database

FAISS enables efficient similarity search over dense vectors.

"""

dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)

index.add(np.array(embeddings))

print("Total Vectors in FAISS:", index.ntotal)

"""## Step 6: Create Retrieval Function

This function retrieves top-k relevant chunks for a given query.

"""

def retrieve(query, top_k=3):
    query_embedding = embedder.encode([query])
    distances, indices = index.search(np.array(query_embedding), top_k)
    results = [chunks[i] for i in indices[0]]
    return results

"""## Step 7: Load Language Model (FLAN-T5)

We use FLAN-T5 for answer generation.

"""

from transformers import pipeline

generator = pipeline(
    "text-generation",
    model="google/flan-t5-base",
    max_new_tokens=256,
    temperature=0.3
)

"""## Step 8: Build RAG Pipeline

The system:
1. Retrieves relevant chunks
2. Injects them into a prompt
3. Generates grounded response

"""

def ask_question(query):
    retrieved_docs = retrieve(query)
    context = "\n".join(retrieved_docs)

    prompt = f"""
    Answer the question using only the context below.

    Context:
    {context}

    Question:
    {query}

    Answer:
    """

    response = generator(prompt)[0]["generated_text"]

    # Remove prompt from response if repeated
    answer = response.replace(prompt, "").strip()

    return answer

"""## Step 9: Test the RAG System

"""

ask_question("What accuracy did the model achieve?")

ask_question("Describe the proposed hybrid model.")

ask_question("What environmental parameters were used?")

ask_question("What are the future enhancements?")

"""# Conclusion

This implementation demonstrates a complete Retrieval-Augmented Generation (RAG) pipeline:

PDF → Chunking → Embeddings → FAISS Index → Retrieval → Context Injection → LLM Answer Generation

The system ensures grounded and context-aware responses derived directly from the research paper.

"""